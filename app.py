import streamlit as st
from ui.sidebar import sidebar
from ui.chat_interface import file_upload_screen, processing_screen, chat_screen
from core.document_processor import process_uploaded_files
from core.embedding_handler import get_embedding_model, get_or_create_vector_store, generate_session_id
from core.llm_handler import get_llm_instance, get_qa_retrieval_chain
from core.chat_history import save_chat_history, load_chat_history

st.set_page_config(page_title="Chatbot T√†i Li·ªáu RAG", layout="wide")
st.title("üí¨ Chatbot H·ªèi ƒê√°p T√†i Li·ªáu (RAG v·ªõi Llama 3)")
def local_css(file_name):
    with open(file_name, encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

local_css("style.css")

# --- Sidebar ---
with st.sidebar:
    new_chat, selected_session = sidebar()

# --- Session State ---
def reset_to_upload():
    st.session_state.state = "upload"
    st.session_state.uploaded_files = None
    st.session_state.processing = False
    st.session_state.vector_store = None
    st.session_state.session_id = None
    st.session_state.file_names = None
    st.session_state.messages = []
    st.session_state.bot_answering = False

def reset_to_chat():
    st.session_state.state = "chatting"
    st.session_state.processing = False
    st.session_state.bot_answering = False

if "state" not in st.session_state:
    st.session_state.state = "upload"
if "uploaded_files" not in st.session_state:
    st.session_state.uploaded_files = None
if "processing" not in st.session_state:
    st.session_state.processing = False
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "session_id" not in st.session_state:
    st.session_state.session_id = None
if "file_names" not in st.session_state:
    st.session_state.file_names = None
if "messages" not in st.session_state:
    st.session_state.messages = []
if "bot_answering" not in st.session_state:
    st.session_state.bot_answering = False

# --- X·ª≠ l√Ω New Chat ho·∫∑c ch·ªçn chat c≈© ---
if new_chat:
    reset_to_upload()
    st.rerun()

if selected_session:
    st.session_state.session_id = selected_session
    st.session_state.messages = load_chat_history(selected_session)
    embedding_model = get_embedding_model()
    from core.embedding_handler import load_vector_store
    # T·∫£i vector store cho session ƒë√£ ch·ªçn
    loaded_vs = load_vector_store(selected_session, embedding_model)
    if loaded_vs:
        st.session_state.vector_store = loaded_vs
        st.session_state.file_names = None # File names kh√¥ng c·∫ßn thi·∫øt khi t·∫£i session c≈©
        st.session_state.state = "chatting"
        st.session_state.bot_answering = False
    else:
        st.error(f"Kh√¥ng th·ªÉ t·∫£i c∆° s·ªü tri th·ª©c cho session '{selected_session}'. C√≥ th·ªÉ ƒë√£ b·ªã x√≥a ho·∫∑c l·ªói. Vui l√≤ng t·∫°o chat m·ªõi.")
        reset_to_upload() # Reset v·ªÅ m√†n h√¨nh upload n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c vector store
    st.rerun()

# --- Giao di·ªán ch√≠nh ---
if st.session_state.state == "upload":
    valid_files, error_files, start_clicked, _ = file_upload_screen(st.session_state.uploaded_files)
    if valid_files:
        st.session_state.uploaded_files = valid_files
    else:
        st.session_state.uploaded_files = None
    if start_clicked and st.session_state.uploaded_files:
        session_id = generate_session_id([f.name for f in st.session_state.uploaded_files])
        st.session_state.session_id = session_id
        st.session_state.state = "processing"
        st.rerun()

elif st.session_state.state == "processing":
    stop_clicked = processing_screen(st.session_state.uploaded_files)
    if stop_clicked:
        reset_to_upload()
        st.rerun()
    else:
        if not st.session_state.vector_store:
            documents, file_names = process_uploaded_files(st.session_state.uploaded_files)
            if documents:
                embedding_model = get_embedding_model()
                # S·ª≠ d·ª•ng st.session_state.session_id ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü b∆∞·ªõc upload
                # B·ªè tham s·ªë save, ƒë·ªÉ d√πng default True trong get_or_create_vector_store
                vector_store, vs_id_saved = get_or_create_vector_store(
                    st.session_state.session_id, 
                    documents, 
                    embedding_model
                )
                if vector_store:
                    st.session_state.vector_store = vector_store
                    st.session_state.file_names = file_names
                    st.session_state.messages = [{"role": "assistant", "content": "T√†i li·ªáu ƒë√£ s·∫µn s√†ng! B·∫°n h√£y ƒë·∫∑t c√¢u h·ªèi."}]
                    # Kh√¥ng c·∫ßn save_vector_store ·ªü ƒë√¢y n·ªØa v√¨ get_or_create_vector_store ƒë√£ x·ª≠ l√Ω
                    reset_to_chat()
                    st.rerun()
                else:
                    st.error("Kh√¥ng th·ªÉ t·∫°o c∆° s·ªü tri th·ª©c.")
                    reset_to_upload()
                    st.rerun()
            else:
                st.error("Kh√¥ng x·ª≠ l√Ω ƒë∆∞·ª£c t√†i li·ªáu.")
                reset_to_upload()
                st.rerun()

elif st.session_state.state == "chatting":
    prompt, send_clicked, stop_clicked = chat_screen(st.session_state.messages, st.session_state.bot_answering)
    if stop_clicked and st.session_state.bot_answering:
        st.session_state.bot_answering = False
        st.session_state.messages.append({"role": "assistant", "content": ":warning: Tr·∫£ l·ªùi ƒë√£ b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng."})
        save_chat_history(st.session_state.session_id, st.session_state.messages)
        st.rerun()
    if send_clicked and not st.session_state.bot_answering and prompt.strip():
        st.session_state.messages.append({"role": "user", "content": prompt.strip()})
        st.session_state.bot_answering = True
        embedding_model = get_embedding_model()
        save_chat_history(st.session_state.session_id, st.session_state.messages)
        st.rerun()
    if st.session_state.bot_answering:
        embedding_model = get_embedding_model()
        vector_store = st.session_state.vector_store
        if not vector_store:
            from core.embedding_handler import load_vector_store
            print(f"[App] Th·ª≠ t·∫£i l·∫°i vector store cho session: {st.session_state.session_id}")
            vector_store = load_vector_store(st.session_state.session_id, embedding_model)
            if vector_store:
                st.session_state.vector_store = vector_store
                print(f"[App] T·∫£i l·∫°i vector store th√†nh c√¥ng.")
            else:
                st.error("L·ªói nghi√™m tr·ªçng: Kh√¥ng th·ªÉ t·∫£i c∆° s·ªü tri th·ª©c cho phi√™n l√†m vi·ªác n√†y. Vui l√≤ng th·ª≠ t·∫°o phi√™n m·ªõi t·ª´ ƒë·∫ßu.")
                st.session_state.bot_answering = False # ƒê·∫£m b·∫£o d·ª´ng bot
                reset_to_upload() # ƒê∆∞a v·ªÅ m√†n h√¨nh upload
                st.rerun() # √Åp d·ª•ng thay ƒë·ªïi state v√† t·∫£i l·∫°i giao di·ªán

        if not st.session_state.vector_store and st.session_state.state == "chatting":
             # ƒêi·ªÅu ki·ªán st.session_state.state == "chatting" ƒë·ªÉ tr√°nh reset n·∫øu ƒë√£ ·ªü upload
            st.error("Kh√¥ng th·ªÉ ti·∫øp t·ª•c phi√™n chat do thi·∫øu c∆° s·ªü tri th·ª©c. Vui l√≤ng b·∫Øt ƒë·∫ßu l·∫°i.")
            reset_to_upload()
            st.rerun()

        # Ch·ªâ ti·∫øp t·ª•c n·∫øu vector_store t·ªìn t·∫°i v√† ƒëang ·ªü state chatting
        if st.session_state.vector_store and st.session_state.state == "chatting":
            llm = get_llm_instance()
            qa_chain = get_qa_retrieval_chain(llm, st.session_state.vector_store.as_retriever(search_kwargs={"k": 3}))
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("Bot ƒëang suy nghƒ©...")
                try:
                    last_user_msg = None
                    for msg in reversed(st.session_state.messages):
                        if msg["role"] == "user":
                            last_user_msg = msg["content"]
                            break
                    response = qa_chain.invoke({"query": last_user_msg})
                    answer = response.get("result", "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi.")
                    sources = response.get("source_documents", [])
                    sources_list = []
                    for src in sources:
                        sources_list.append({
                            "source": src.metadata.get("source", "N/A"),
                            "chunk_id": src.metadata.get("chunk_id", "N/A"),
                            "content": src.page_content.replace("\n", " ")
                        })
                    message_placeholder.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources_list})
                    save_chat_history(st.session_state.session_id, st.session_state.messages)
                    st.session_state.bot_answering = False
                    st.rerun()
                except Exception as e:
                    error_message = f"ƒê√£ x·∫£y ra l·ªói: {e}"
                    message_placeholder.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})
                    save_chat_history(st.session_state.session_id, st.session_state.messages)
                    st.session_state.bot_answering = False
                    st.rerun()