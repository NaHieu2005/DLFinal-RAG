import streamlit as st
import os
from dotenv import load_dotenv
import pickle # N·∫øu b·∫°n l∆∞u document objects
import re

# Import c√°c l·ªõp LangChain c·∫ßn thi·∫øt
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# Th∆∞ vi·ªán ƒë·ªçc PDF (v√≠ d·ª•)
import PyPDF2 # Ho·∫∑c pdfplumber

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng (n·∫øu c√≥, v√≠ d·ª• cho API key n·∫øu b·∫°n chuy·ªÉn sang LLM API kh√°c)
# load_dotenv() # Hi·ªán t·∫°i kh√¥ng c·∫ßn cho Ollama c·ª•c b·ªô

# --- C·∫•u h√¨nh c·ªë ƒë·ªãnh (c√≥ th·ªÉ ƒë∆∞a ra ngo√†i ho·∫∑c l√†m c·∫•u h√¨nh ng∆∞·ªùi d√πng) ---
embedding_model_name = "bkai-foundation-models/vietnamese-bi-encoder"
ollama_model_name = "llama3:8b-instruct-q4_0"

# --- H√†m x·ª≠ l√Ω t√†i li·ªáu ---
def process_uploaded_file(uploaded_file):
    """ƒê·ªçc, l√†m s·∫°ch v√† chia chunk t√†i li·ªáu ƒë∆∞·ª£c t·∫£i l√™n."""
    text = ""
    if uploaded_file is not None:
        try:
            file_extension = os.path.splitext(uploaded_file.name)[1].lower()
            if file_extension == ".txt":
                text = str(uploaded_file.read(), "utf-8")
            elif file_extension == ".pdf":
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text()
            else:
                st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i file .txt ho·∫∑c .pdf.")
                return None
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc file: {e}")
            return None

        # L√†m s·∫°ch vƒÉn b·∫£n (b·∫°n c√≥ th·ªÉ th√™m c√°c b∆∞·ªõc l√†m s·∫°ch chi ti·∫øt h∆°n ·ªü ƒë√¢y)
        text = re.sub(r'\s+', ' ', text).strip() # V√≠ d·ª• l√†m s·∫°ch c∆° b·∫£n

        # Chia chunk
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=300,
            length_function=len,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        # chunks_text = text_splitter.split_text(text)
        # documents = [Document(page_content=t) for t in chunks_text] # T·∫°o Document objects ƒë∆°n gi·∫£n
        documents = text_splitter.create_documents([text]) # C√°ch n√†y t·ªët h∆°n
        for i, doc in enumerate(documents): # Th√™m metadata c∆° b·∫£n
            doc.metadata["source"] = uploaded_file.name
            doc.metadata["chunk_id"] = i
        return documents
    return None

# --- H√†m t·∫°o ho·∫∑c t·∫£i Vector Store ---
# Cache ƒë·ªÉ kh√¥ng ph·∫£i t·∫°o l·∫°i embedding v√† vector store m·ªói l·∫ßn t∆∞∆°ng t√°c
@st.cache_resource(show_spinner="ƒêang x·ª≠ l√Ω t√†i li·ªáu v√† t·∫°o c∆° s·ªü tri th·ª©c...")
def get_vector_store(_documents, _embedding_model_name): # Th√™m _ tr∆∞·ªõc embedding_model_name ƒë·ªÉ Streamlit cache ƒë√∫ng
    """T·∫°o FAISS vector store t·ª´ c√°c document ƒë√£ chia chunk."""
    if _documents:
        try:
            embeddings_model_instance = HuggingFaceEmbeddings(
                model_name=_embedding_model_name,
                model_kwargs={'device': 'cuda'}
            )
            vector_store_instance = FAISS.from_documents(_documents, embeddings_model_instance)
            return vector_store_instance
        except Exception as e:
            st.error(f"L·ªói khi t·∫°o vector store: {e}")
            return None
    return None

# --- H√†m kh·ªüi t·∫°o LLM v√† QA Chain ---
# Cache ƒë·ªÉ kh√¥ng ph·∫£i load l·∫°i LLM m·ªói l·∫ßn
@st.cache_resource(show_spinner="ƒêang kh·ªüi t·∫°o m√¥ h√¨nh AI...")
def get_qa_chain(_ollama_model_name): # Th√™m _ tr∆∞·ªõc ollama_model_name
    """Kh·ªüi t·∫°o LLM v√† RetrievalQA chain."""
    try:
        llm_instance = ChatOllama(
            model=_ollama_model_name,
            temperature=0.2,
        )

        prompt_template_str = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, chuy√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
H√£y s·ª≠ d·ª•ng c√°c ƒëo·∫°n th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt ho·∫∑c th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu. ƒê·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.
Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† m·∫°ch l·∫°c.

Th√¥ng tin tham kh·∫£o:
{context}<|eot_id|><|start_header_id|>user<|end_header_id|>
C√¢u h·ªèi: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
C√¢u tr·∫£ l·ªùi h·ªØu √≠ch:"""

        PROMPT = PromptTemplate(
            template=prompt_template_str, input_variables=["context", "question"]
        )
        chain_type_kwargs = {"prompt": PROMPT}

        # Retriever s·∫Ω ƒë∆∞·ª£c g√°n sau khi vector_store ƒë∆∞·ª£c t·∫°o
        # N√™n ch√∫ng ta kh√¥ng t·∫°o qa_chain ho√†n ch·ªânh ·ªü ƒë√¢y ngay
        return llm_instance, chain_type_kwargs
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o LLM: {e}")
        return None, None


# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="Chatbot T√†i Li·ªáu RAG", layout="wide")
st.title("üí¨ Chatbot H·ªèi ƒê√°p T√†i Li·ªáu (RAG v·ªõi Llama 3)")
st.markdown("T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ( .txt ho·∫∑c .pdf) v√† ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung ƒë√≥.")

# --- Thanh b√™n (Sidebar) cho vi·ªác t·∫£i file ---
with st.sidebar:
    st.header("üìÅ T·∫£i T√†i Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file .txt ho·∫∑c .pdf", type=["txt", "pdf"])

    if uploaded_file:
        st.success(f"ƒê√£ t·∫£i l√™n file: {uploaded_file.name}")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt t√†i li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# --- X·ª≠ l√Ω v√† kh·ªüi t·∫°o ---
documents = None
vector_store = None
llm, chain_type_kwargs = None, None

if uploaded_file:
    documents = process_uploaded_file(uploaded_file)
    if documents:
        vector_store = get_vector_store(documents, embedding_model_name) # Truy·ªÅn t√™n model v√†o
        if vector_store:
            llm, chain_type_kwargs = get_qa_chain(ollama_model_name) # Truy·ªÅn t√™n model v√†o

# --- Kh·ªüi t·∫°o l·ªãch s·ª≠ chat trong session state ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! H√£y t·∫£i t√†i li·ªáu l√™n v√† ƒë·∫∑t c√¢u h·ªèi cho t√¥i nh√©."}]

# Hi·ªÉn th·ªã c√°c tin nh·∫Øn ƒë√£ c√≥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("Xem ngu·ªìn tham kh·∫£o"):
                for i, source in enumerate(message["sources"]):
                    st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')}, Chunk ID: {source.metadata.get('chunk_id', 'N/A')})")
                    st.markdown(source.page_content.replace("\n", " ")[:300] + "...")


# --- Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng ---
if prompt := st.chat_input("C√¢u h·ªèi c·ªßa b·∫°n v·ªÅ t√†i li·ªáu..."):
    if not uploaded_file or not vector_store or not llm:
        st.warning("Vui l√≤ng t·∫£i l√™n v√† x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")
    else:
        # Th√™m tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ chat
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # T·∫°o QA chain v·ªõi retriever ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True,
            chain_type_kwargs=chain_type_kwargs
        )

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("Bot ƒëang suy nghƒ©...")
            try:
                response = qa_chain.invoke({"query": prompt})
                answer = response.get("result", "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi.")
                if hasattr(answer, 'content'): # N·∫øu k·∫øt qu·∫£ l√† AIMessage
                    answer = answer.content

                message_placeholder.markdown(answer)
                # L∆∞u ngu·ªìn tham kh·∫£o
                sources = response.get("source_documents", [])
                st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})

                # Hi·ªÉn th·ªã ngu·ªìn ngay d∆∞·ªõi c√¢u tr·∫£ l·ªùi (t√πy ch·ªçn)
                if sources:
                    with st.expander("Xem ngu·ªìn tham kh·∫£o cho c√¢u tr·∫£ l·ªùi n√†y"):
                        for i, source in enumerate(sources):
                            st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')}, Chunk ID: {source.metadata.get('chunk_id', 'N/A')})")
                            st.markdown(source.page_content.replace("\n", " ")[:300] + "...")


            except Exception as e:
                error_message = f"ƒê√£ x·∫£y ra l·ªói: {e}"
                message_placeholder.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})