import streamlit as st
import os
import pickle 
import re
import uuid 

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain.storage import InMemoryStore
from langchain_core.runnables import RunnableLambda 

# Th∆∞ vi·ªán ƒë·ªçc PDF
import PyPDF2

COHERE_API_KEY = "LSsVgNhEJmAWepOBmRNcSwXABh18VtmMXWYGLpD2" 

# --- C·∫•u h√¨nh c·ªë ƒë·ªãnh ---
embedding_model_name = "bkai-foundation-models/vietnamese-bi-encoder"
ollama_model_name = "llama3:8b-instruct-q4_0"

# --- H√†m x·ª≠ l√Ω t√†i li·ªáu ---
def process_uploaded_file(uploaded_file):
    """ƒê·ªçc file v√† tr·∫£ v·ªÅ m·ªôt list ch·ª©a Document object th√¥."""
    text = ""
    if uploaded_file is not None:
        try:
            file_extension = os.path.splitext(uploaded_file.name)[1].lower()
            if file_extension == ".txt":
                text = str(uploaded_file.read(), "utf-8")
            elif file_extension == ".pdf":
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text()
            else:
                st.error("ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Vui l√≤ng t·∫£i file .txt ho·∫∑c .pdf.")
                return None
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc file: {e}")
            return None
        documents = [Document(page_content=text, metadata={"source": uploaded_file.name})]
        return documents
    return None

# --- H√†m kh·ªüi t·∫°o LLM ---
@st.cache_resource(show_spinner="ƒêang kh·ªüi t·∫°o m√¥ h√¨nh AI...")
def get_llm_and_prompt(_ollama_model_name):
    """Kh·ªüi t·∫°o LLM v√† Prompt Template."""
    try:
        llm_instance = ChatOllama(model=_ollama_model_name, temperature=0.2)
        prompt_template_str = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI h·ªØu √≠ch, chuy√™n tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.
H√£y s·ª≠ d·ª•ng c√°c ƒëo·∫°n th√¥ng tin sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt ho·∫∑c th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu. ƒê·ª´ng c·ªë b·ªãa ra c√¢u tr·∫£ l·ªùi.
Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát m·ªôt c√°ch r√µ r√†ng v√† m·∫°ch l·∫°c.

Th√¥ng tin tham kh·∫£o:
{context}<|eot_id|><|start_header_id|>user<|end_header_id|>
C√¢u h·ªèi: {question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
C√¢u tr·∫£ l·ªùi h·ªØu √≠ch:"""
        PROMPT = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])
        chain_type_kwargs = {"prompt": PROMPT}
        return llm_instance, chain_type_kwargs
    except Exception as e:
        st.error(f"L·ªói khi kh·ªüi t·∫°o LLM: {e}")
        return None, None

# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="Chatbot T√†i Li·ªáu RAG", layout="wide")
st.title("üí¨ Chatbot H·ªèi ƒê√°p T√†i Li·ªáu N√¢ng Cao")
st.markdown("T·∫£i l√™n t√†i li·ªáu c·ªßa b·∫°n ( .txt ho·∫∑c .pdf) v√† ƒë·∫∑t c√¢u h·ªèi v·ªÅ n·ªôi dung ƒë√≥.")

# --- Thanh b√™n (Sidebar) ---
with st.sidebar:
    st.header("üìÅ T·∫£i T√†i Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file .txt ho·∫∑c .pdf", type=["txt", "pdf"])
    if uploaded_file:
        st.success(f"ƒê√£ t·∫£i l√™n file: {uploaded_file.name}")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n m·ªôt t√†i li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# --- X·ª≠ l√Ω v√† kh·ªüi t·∫°o ---
# Kh·ªëi n√†y ƒë∆∞·ª£c t√°i c·∫•u tr√∫c ho√†n to√†n ƒë·ªÉ t√≠ch h·ª£p t·∫•t c·∫£ c√°c k·ªπ thu·∫≠t
@st.cache_resource(show_spinner="ƒêang x·ª≠ l√Ω t√†i li·ªáu v√† x√¢y d·ª±ng pipeline RAG...")
def build_full_rag_pipeline(_raw_documents):
    # --- 1: LOGIC C·ª¶A PARENT DOCUMENT ---
    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100)

    parent_chunks = parent_splitter.split_documents(_raw_documents)
    
    docstore = InMemoryStore()
    parent_chunk_ids = [str(uuid.uuid4()) for _ in parent_chunks]
    docstore.mset(list(zip(parent_chunk_ids, parent_chunks)))

    child_chunks = []
    for i, p_chunk in enumerate(parent_chunks):
        _child_chunks = child_splitter.split_documents([p_chunk])
        for c_chunk in _child_chunks:
            c_chunk.metadata["parent_id"] = parent_chunk_ids[i]
        child_chunks.extend(_child_chunks)

    # --- 2: LOGIC C·ª¶A HYBRID SEARCH (tr√™n chunk con) ---
    embeddings_model_instance = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cuda'})
    vectorstore = FAISS.from_documents(child_chunks, embeddings_model_instance)
    
    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})
    bm25_retriever = BM25Retriever.from_documents(child_chunks)
    bm25_retriever.k = 10
    
    hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])

    # --- 3: K·∫æT H·ª¢P PARENT RETRIEVAL V√Ä HYBRID SEARCH ---
    def get_parent_chunks(child_docs):
        parent_ids = {doc.metadata["parent_id"] for doc in child_docs}
        return [doc for doc in docstore.mget(list(parent_ids)) if doc is not None]

    parent_retriever_chain = hybrid_retriever | RunnableLambda(get_parent_chunks)

    # --- 4: LOGIC C·ª¶A RE-RANKING (tr√™n chunk cha) ---
    reranker = CohereRerank(cohere_api_key=COHERE_API_KEY, model="rerank-multilingual-v3.0", top_n=3)
    final_retriever = ContextualCompressionRetriever(base_compressor=reranker, base_retriever=parent_retriever_chain)
    
    return final_retriever

retriever = None
llm, chain_type_kwargs = None, None

if uploaded_file:
    raw_documents = process_uploaded_file(uploaded_file)
    if raw_documents:
        retriever = build_full_rag_pipeline(raw_documents)
        llm, chain_type_kwargs = get_llm_and_prompt(ollama_model_name)

# --- Kh·ªüi t·∫°o v√† hi·ªÉn th·ªã l·ªãch s·ª≠ chat ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Xin ch√†o! H√£y t·∫£i t√†i li·ªáu l√™n v√† ƒë·∫∑t c√¢u h·ªèi cho t√¥i nh√©."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("Xem ngu·ªìn tham kh·∫£o"):
                for i, source in enumerate(message["sources"]):
                    st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')})")
                    st.markdown(source.page_content.replace("\n", " "))

# --- Nh·∫≠n input t·ª´ ng∆∞·ªùi d√πng ---
if prompt := st.chat_input("C√¢u h·ªèi c·ªßa b·∫°n v·ªÅ t√†i li·ªáu..."):
    if not uploaded_file or not retriever or not llm:
        st.warning("Vui l√≤ng t·∫£i l√™n v√† x·ª≠ l√Ω t√†i li·ªáu tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")
    else:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever, 
            return_source_documents=True,
            chain_type_kwargs=chain_type_kwargs
        )

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("Bot ƒëang t√¨m ki·∫øm v√† suy lu·∫≠n...")
            try:
                response = qa_chain.invoke({"query": prompt})
                answer = response.get("result", "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi.")
                message_placeholder.markdown(answer)
                
                sources = response.get("source_documents", [])
                st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})

                if sources:
                    with st.expander("Xem ngu·ªìn tham kh·∫£o cho c√¢u tr·∫£ l·ªùi n√†y"):
                        for i, source in enumerate(sources):
                            st.caption(f"Ngu·ªìn {i+1} (T·ª´: {source.metadata.get('source', 'N/A')})")
                            st.markdown(source.page_content.replace("\n", " "))

            except Exception as e:
                error_message = f"ƒê√£ x·∫£y ra l·ªói: {e}"
                message_placeholder.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})